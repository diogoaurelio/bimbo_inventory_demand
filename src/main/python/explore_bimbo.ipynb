{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "#from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.files import SparkFiles\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.accumulators import Accumulator, AccumulatorParam\n",
    "from pyspark.broadcast import Broadcast\n",
    "from pyspark.serializers import MarshalSerializer, PickleSerializer\n",
    "from pyspark.status import *\n",
    "from pyspark.profiler import Profiler, BasicProfiler\n",
    "\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import rank, min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "from datetime import timedelta as td\n",
    "from dateutil import rrule\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "# for notebook:\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier  #GBM algorithm\n",
    "\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(path, file_name, nrows=None, verbose=True):\n",
    "    \"\"\"\n",
    "    convenience func for printing\n",
    "    side effects\n",
    "    :param path:\n",
    "    :param file_name:\n",
    "    :param nrows:\n",
    "    :return:\n",
    "            (pandas.dataframe)\n",
    "    \"\"\"\n",
    "    data_path = os.path.join(path, file_name)\n",
    "    if verbose:\n",
    "        print('\\n#################################')\n",
    "        print('Loading data from {0}...'.format(data_path))\n",
    "    data = pd.read_csv(data_path, nrows=nrows)\n",
    "    if verbose:\n",
    "        print('Dataset num rows: {0}, num cols: {1}'\n",
    "              .format(data.shape[0],data.shape[1]))\n",
    "        print('Columns: {}'.format(list(data.columns.values)))\n",
    "        print('Head: ')\n",
    "        print(data.head())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data..\n",
      "\n",
      "#################################\n",
      "Loading data from /mnt/tests/train.csv...\n",
      "Dataset num rows: 1000, num cols: 11\n",
      "Columns: ['Semana', 'Agencia_ID', 'Canal_ID', 'Ruta_SAK', 'Cliente_ID', 'Producto_ID', 'Venta_uni_hoy', 'Venta_hoy', 'Dev_uni_proxima', 'Dev_proxima', 'Demanda_uni_equil']\n",
      "Head: \n",
      "   Semana  Agencia_ID  Canal_ID  Ruta_SAK  Cliente_ID  Producto_ID  \\\n",
      "0       3        1110         7      3301       15766         1212   \n",
      "1       3        1110         7      3301       15766         1216   \n",
      "2       3        1110         7      3301       15766         1238   \n",
      "3       3        1110         7      3301       15766         1240   \n",
      "4       3        1110         7      3301       15766         1242   \n",
      "\n",
      "   Venta_uni_hoy  Venta_hoy  Dev_uni_proxima  Dev_proxima  Demanda_uni_equil  \n",
      "0              3      25.14                0          0.0                  3  \n",
      "1              4      33.52                0          0.0                  4  \n",
      "2              4      39.32                0          0.0                  4  \n",
      "3              4      33.52                0          0.0                  4  \n",
      "4              3      22.92                0          0.0                  3  \n",
      "\n",
      "#################################\n",
      "Loading data from /mnt/tests/cliente_tabla.csv...\n",
      "Dataset num rows: 935362, num cols: 2\n",
      "Columns: ['Cliente_ID', 'NombreCliente']\n",
      "Head: \n",
      "   Cliente_ID                            NombreCliente\n",
      "0           0                               SIN NOMBRE\n",
      "1           1                         OXXO XINANTECATL\n",
      "2           2                               SIN NOMBRE\n",
      "3           3                                EL MORENO\n",
      "4           4  SDN SER  DE ALIM  CUERPO SA CIA  DE INT\n",
      "\n",
      "#################################\n",
      "Loading data from /mnt/tests/producto_tabla.csv...\n",
      "Dataset num rows: 2592, num cols: 2\n",
      "Columns: ['Producto_ID', 'NombreProducto']\n",
      "Head: \n",
      "   Producto_ID                          NombreProducto\n",
      "0            0                       NO IDENTIFICADO 0\n",
      "1            9               Capuccino Moka 750g NES 9\n",
      "2           41  Bimbollos Ext sAjonjoli 6p 480g BIM 41\n",
      "3           53          Burritos Sincro 170g CU LON 53\n",
      "4           72     Div Tira Mini Doradita 4p 45g TR 72\n",
      "\n",
      "#################################\n",
      "Loading data from /mnt/tests/town_state.csv...\n",
      "Dataset num rows: 790, num cols: 3\n",
      "Columns: ['Agencia_ID', 'Town', 'State']\n",
      "Head: \n",
      "   Agencia_ID                   Town             State\n",
      "0        1110     2008 AG. LAGO FILT      MÉXICO, D.F.\n",
      "1        1111  2002 AG. AZCAPOTZALCO      MÉXICO, D.F.\n",
      "2        1112    2004 AG. CUAUTITLAN  ESTADO DE MÉXICO\n",
      "3        1113     2008 AG. LAGO FILT      MÉXICO, D.F.\n",
      "4        1114   2029 AG.IZTAPALAPA 2      MÉXICO, D.F.\n",
      "\n",
      "#################################\n",
      "Loading data from /mnt/tests/test.csv...\n",
      "Dataset num rows: 6999251, num cols: 7\n",
      "Columns: ['id', 'Semana', 'Agencia_ID', 'Canal_ID', 'Ruta_SAK', 'Cliente_ID', 'Producto_ID']\n",
      "Head: \n",
      "   id  Semana  Agencia_ID  Canal_ID  Ruta_SAK  Cliente_ID  Producto_ID\n",
      "0   0      11        4037         1      2209     4639078        35305\n",
      "1   1      11        2237         1      1226     4705135         1238\n",
      "2   2      10        2045         1      2831     4549769        32940\n",
      "3   3      11        1227         1      4448     4717855        43066\n",
      "4   4      11        1219         1      1130      966351         1277\n"
     ]
    }
   ],
   "source": [
    "DATA = '/mnt/tests'\n",
    "\n",
    "train_path = os.path.join(DATA, 'train.csv')\n",
    "test_path = os.path.join(DATA, 'test.csv')\n",
    "client_data_path = os.path.join(DATA, 'cliente_tabla.csv')\n",
    "product_data_path = os.path.join(DATA, 'producto_tabla.csv')\n",
    "town_state_path = os.path.join(DATA, 'town_state.csv')\n",
    "\n",
    "print('Loading data..')\n",
    "df_train = load_data(path=DATA, file_name='train.csv', nrows=10**3)\n",
    "df_client = load_data(path=DATA, file_name='cliente_tabla.csv')\n",
    "df_prod = load_data(path=DATA, file_name='producto_tabla.csv')\n",
    "df_town = load_data(path=DATA, file_name='town_state.csv')\n",
    "df_test = load_data(path=DATA, file_name='test.csv')\n",
    "ids = df_test['id']\n",
    "df_test = df_test.drop(['id'], axis=1)\n",
    "\n",
    "target = 'Demanda_uni_equil'\n",
    "indep_vars = list(df_test.columns.values)\n",
    "y = df_train[target]\n",
    "X = df_train[indep_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_gbm(X_train,  y_train, grid_search=False, verbose=True,\n",
    "              min_samples_split=None, min_samples_leaf=50, max_depth=8,\n",
    "              max_features='sqrt', sub_sample=0.8, n_estimators=100,\n",
    "              learning_rate=0.1, random_state=10, param_grid=None,\n",
    "              ):\n",
    "    \"\"\"\n",
    "\n",
    "    :param X_train:\n",
    "    :param indep_vars:\n",
    "    :param dep_var:\n",
    "    :param verbose:\n",
    "    :param min_samples_leaf: (int) prevent overfitting, intuition based value..\n",
    "    :param max_depth: (int) 8 # 5 -8, based on number of features and dataset size\n",
    "    :param max_features: (str) 'sqrt' # general rule of thumb: sqrt(n_samples)\n",
    "    :param sub_sample: (float) fraction of observations to be selected for each tree (0.8 commonly used value)\n",
    "    :param n_estimators: (int) number of sequential trees to be modeled\n",
    "    :param learning_rate: (float)\n",
    "    :param random_state: (int)\n",
    "    :param param_grid: (dict)\n",
    "    :return:\n",
    "            (model)\n",
    "    \"\"\"\n",
    "    n_samples = X_train.shape[0]\n",
    "    if not min_samples_split:\n",
    "        min_samples_split = n_samples * .01 # prevent overfitting, general rule of thumb: 0.5 - 1%\n",
    "\n",
    "    gbm = GradientBoostingClassifier(\n",
    "        min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n",
    "        max_depth=max_depth, max_features=max_features, learning_rate=learning_rate,\n",
    "        n_estimators=n_estimators, subsample=sub_sample, random_state=random_state)\n",
    "    model = gbm\n",
    "    if grid_search:\n",
    "        if not param_grid:\n",
    "            #param_grid = {'n_estimators':range(20,81,10)}\n",
    "            param_grid = [{'n_estimators':range(20,81,10)}]\n",
    "        model = GridSearchCV(\n",
    "            estimator=gbm,\n",
    "            param_grid=param_grid,\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=4,\n",
    "            iid=False,\n",
    "            cv=5)\n",
    "    if verbose:\n",
    "        print('Starting to train model...')\n",
    "    model.fit(X_train, y_train)\n",
    "    if grid_search and verbose:\n",
    "        print('Model Grid scores: {0}, best params: {1}, best score: {2}'\n",
    "              .format(model.grid_scores_, model.best_params_, model.best_score_))\n",
    "    if verbose:\n",
    "        print('Finished training.')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gbm_predict(X_train, y_train, X_test, y_test, indep_vars, grid_search=True,\n",
    "                cv=True, verbose=True, cv_folds=5, scoring='roc_auc',\n",
    "                submit=False, version='001', y_label='Demanda_uni_equil',\n",
    "                ids=None):\n",
    "\n",
    "    # Train\n",
    "    model = train_gbm(X_train,  y_train, grid_search=grid_search, verbose=verbose)\n",
    "    # predict on train\n",
    "    if verbose:\n",
    "        print('Starting predictions ...')\n",
    "    test_pred = model.predict(X_test)\n",
    "\n",
    "    if scoring=='roc_auc':\n",
    "        # probability\n",
    "        test_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    if cv:\n",
    "        cv_score = cross_validation.cross_val_score(model, X_test,\n",
    "                                                    y_test, cv=cv_folds,\n",
    "                                                    scoring=scoring)\n",
    "    if verbose and not submit:\n",
    "        print(\"\\nModel Report\")\n",
    "        if scoring=='roc_auc':\n",
    "            print(\"Accuracy : %.4g\" % metrics.accuracy_score(X_test.values, test_pred))\n",
    "            print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(X_test, test_pred_prob))\n",
    "            if cv:\n",
    "                print(\"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\"\n",
    "                      % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n",
    "        # feature relevance\n",
    "        predictive_relavance = pd.Series(model.feature_importances_, indep_vars).sort_values(ascending=False)\n",
    "        predictive_relavance.plot(kind='bar', title='Feature relevance')\n",
    "        plt.ylabel('Feature relevance score')\n",
    "\n",
    "    if submit:\n",
    "        submission = pd.DataFrame({'id':ids, y_label: test_pred})\n",
    "        submission.to_csv('submission_gbm_{}.csv'.format(version), index=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----------------------\n",
      "Finally, loading Kaggle test set to perform predictions...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Parameter values should be a list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-813caa2e78ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m                     \u001b[0mindep_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindep_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                     \u001b[0mcv_folds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean_squared_error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubmit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                     version='001', y_label=target)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-a0b3659353ce>\u001b[0m in \u001b[0;36mgbm_predict\u001b[1;34m(X_train, y_train, X_test, y_test, indep_vars, grid_search, cv, verbose, cv_folds, scoring, submit, version, y_label, ids)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# Train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_gbm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;31m# predict on train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-c436b6c10767>\u001b[0m in \u001b[0;36mtrain_gbm\u001b[1;34m(X_train, y_train, grid_search, verbose, min_samples_split, min_samples_leaf, max_depth, max_features, sub_sample, n_estimators, learning_rate, random_state, param_grid)\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0miid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             cv=5)\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Starting to train model...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/hadoop/miniconda/envs/spark/lib/python3.4/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, estimator, param_grid, scoring, fit_params, n_jobs, iid, refit, cv, verbose, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    785\u001b[0m             refit, cv, verbose, pre_dispatch, error_score)\n\u001b[0;32m    786\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0m_check_param_grid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/hadoop/miniconda/envs/spark/lib/python3.4/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36m_check_param_grid\u001b[1;34m(param_grid)\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[0mcheck\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcheck\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Parameter values should be a list.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Parameter values should be a list."
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\n",
    "\n",
    "# w/o grid search\n",
    "# gbm_10 = gbm_predict(X_train=X_train, y_train=y_train, X_test=X_test, y_test=np.array(y_test),\n",
    "#                     indep_vars=indep_vars, grid_search=False, cv=True, verbose=True,\n",
    "#                     cv_folds=5, scoring='mean_squared_error', y_label=target)\n",
    "print('\\n\\n----------------------')\n",
    "print('Finally, loading Kaggle test set to perform predictions...')\n",
    "gbm_11 = gbm_predict(X_train=X_train, y_train=y_train, X_test=df_test, y_test=np.array(y_test),\n",
    "                    indep_vars=indep_vars, grid_search=True, cv=True, verbose=True,\n",
    "                    cv_folds=5, scoring='mean_squared_error', submit=True, ids=ids,\n",
    "                    version='001', y_label=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the big guns - enter spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialize a configuration object ..\n",
    "MY_NAME='diogo' # change this\n",
    "conf = SparkConf().setMaster(\"yarn-client\").setAppName(\"bimbo - {}\".format(MY_NAME)) \\\n",
    ".set(\"spark.driver.memory\", \"2g\").set(\"spark.executor.memory\", \"2g\") \\\n",
    ".set(\"spark.executor.instances\", \"2\").set(\"spark.dynamicAllocation.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# .. and pass it to a new SparkContext \n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "#initialize hiveContext\n",
    "#sqlContext = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_from_s3=False\n",
    "if load_from_s3:\n",
    "    s3_train = 's3n://bonial-jobs-stage/notebooks/training/grupo_bimbo/train.csv'\n",
    "    s3_test = 's3n://bonial-jobs-stage/notebooks/training/grupo_bimbo/test.csv'\n",
    "    s3_client = 's3n://bonial-jobs-stage/notebooks/training/grupo_bimbo/cliente_tabla.csv'\n",
    "    s3_prod = 's3n://bonial-jobs-stage/notebooks/training/grupo_bimbo/producto_tabla.csv'\n",
    "    s3_town = 's3n://bonial-jobs-stage/notebooks/training/grupo_bimbo/town_state.csv'\n",
    "else:\n",
    "    s3_train = 'mnt/tests/train.csv'\n",
    "    s3_test = 'mnt/tests/test.csv'\n",
    "    s3_client = 'mnt/tests/cliente_tabla.csv'\n",
    "    s3_prod = 'mnt/tests/producto_tabla.csv'\n",
    "    s3_town = 'mnt/tests/town_state.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets from hdfs\n",
      "Loading train set...\n",
      "finished loading\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- Semana: string (nullable = true)\n",
      " |-- Agencia_ID: string (nullable = true)\n",
      " |-- Canal_ID: string (nullable = true)\n",
      " |-- Ruta_SAK: string (nullable = true)\n",
      " |-- Cliente_ID: string (nullable = true)\n",
      " |-- Producto_ID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading datasets from hdfs')\n",
    "print('Loading train set...')\n",
    "df_train = sqlContext.read \\\n",
    "    .format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(s3_train)\n",
    "#df_train.select(\"*\").write.save(\"{}.parquet\".format(\"train_bimbo\"), format=\"parquet\")\n",
    "print('finished loading')\n",
    "df_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test set\n",
      "finished loading\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- Semana: string (nullable = true)\n",
      " |-- Agencia_ID: string (nullable = true)\n",
      " |-- Canal_ID: string (nullable = true)\n",
      " |-- Ruta_SAK: string (nullable = true)\n",
      " |-- Cliente_ID: string (nullable = true)\n",
      " |-- Producto_ID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading test set...')\n",
    "df_test = sqlContext.read \\\n",
    "    .format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(s3_test)\n",
    "#df_test.select(\"*\").write.save(\"{}.parquet\".format(\"test_bimbo\"), format=\"parquet\")\n",
    "print('finished loading')\n",
    "df_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading client set...\n",
      "finished loading\n",
      "root\n",
      " |-- Cliente_ID: string (nullable = true)\n",
      " |-- NombreCliente: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading client set...')\n",
    "df_client = sqlContext.read \\\n",
    "    .format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(s3_client)\n",
    "#df_client.select(\"*\").write.save(\"{}.parquet\".format(\"client_bimbo\"), format=\"parquet\")\n",
    "print('finished loading')\n",
    "df_client.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished loading\n",
      "root\n",
      " |-- Producto_ID: string (nullable = true)\n",
      " |-- NombreProducto: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_prod = sqlContext.read \\\n",
    "    .format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(s3_prod)   \n",
    "#df_prod.select(\"*\").write.save(\"{}.parquet\".format(\"prod_bimbo\"), format=\"parquet\")\n",
    "print('finished loading')\n",
    "df_prod.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished loading\n",
      "root\n",
      " |-- Agencia_ID: string (nullable = true)\n",
      " |-- Town: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_town = sqlContext.read \\\n",
    "    .format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(s3_town)\n",
    "#df_town.select(\"*\").write.save(\"{}.parquet\".format(\"town_bimbo\"), format=\"parquet\")\n",
    "print('finished loading')\n",
    "df_town.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_train = sqlContext.read.parquet('train_bimbo')\n",
    "#df_test = sqlContext.read.parquet('test_bimbo')\n",
    "#df_client = sqlContext.read.parquet('client_bimbo')\n",
    "#df_prod = sqlContext.read.parquet('prod_bimbo')\n",
    "#df_town = sqlContext.read.parquet('town_bimbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing head\n",
      "root\n",
      " |-- Semana: string (nullable = true)\n",
      " |-- Agencia_ID: string (nullable = true)\n",
      " |-- Canal_ID: string (nullable = true)\n",
      " |-- Ruta_SAK: string (nullable = true)\n",
      " |-- Cliente_ID: string (nullable = true)\n",
      " |-- Producto_ID: string (nullable = true)\n",
      " |-- Venta_uni_hoy: string (nullable = true)\n",
      " |-- Venta_hoy: string (nullable = true)\n",
      " |-- Dev_uni_proxima: string (nullable = true)\n",
      " |-- Dev_proxima: string (nullable = true)\n",
      " |-- Demanda_uni_equil: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-77cb1037ac6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Showing head'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;33m+\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \"\"\"\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m    813\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[1;32m/usr/lib/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry)\u001b[0m\n\u001b[0;32m    624\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 626\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_give_back_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m             \u001b[1;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/hadoop/miniconda/envs/spark/lib/python3.4/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Showing head')\n",
    "df_train.printSchema()\n",
    "df_train.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-102fc93efe29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpd_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpd_client\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpd_prod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_prod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpd_town\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_town\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1379\u001b[0m         \"\"\"\n\u001b[0;32m   1380\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1381\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m     \u001b[1;31m##########################################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m         \"\"\"\n\u001b[0;32m    279\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m    813\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[1;32m/usr/lib/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry)\u001b[0m\n\u001b[0;32m    624\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 626\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_give_back_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m             \u001b[1;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/hadoop/miniconda/envs/spark/lib/python3.4/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
