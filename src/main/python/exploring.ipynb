{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "from datetime import timedelta as td\n",
    "from dateutil import rrule\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For local ML\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier  #GBM algorithm\n",
    "\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For distributed computing (with Spark)\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "#from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.files import SparkFiles\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.accumulators import Accumulator, AccumulatorParam\n",
    "from pyspark.broadcast import Broadcast\n",
    "from pyspark.serializers import MarshalSerializer, PickleSerializer\n",
    "from pyspark.status import *\n",
    "from pyspark.profiler import Profiler, BasicProfiler\n",
    "\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import rank, min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, file_name, nrows=None, verbose=True):\n",
    "    \"\"\"\n",
    "    convenience func for printing\n",
    "    side effects\n",
    "    :param path:\n",
    "    :param file_name:\n",
    "    :param nrows:\n",
    "    :return:\n",
    "            (pandas.dataframe)\n",
    "    \"\"\"\n",
    "    data_path = os.path.join(path, file_name)\n",
    "    if verbose:\n",
    "        print('\\n#################################')\n",
    "        print('Loading data from {0}...'.format(data_path))\n",
    "    data = pd.read_csv(data_path, nrows=nrows)\n",
    "    if verbose:\n",
    "        print('Dataset num rows: {0}, num cols: {1}'\n",
    "              .format(data.shape[0],data.shape[1]))\n",
    "        print('Columns: {}'.format(list(data.columns.values)))\n",
    "        print('Head: ')\n",
    "        print(data.head())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = '/mnt/tests'\n",
    "\n",
    "train_path = os.path.join(DATA, 'train.csv')\n",
    "test_path = os.path.join(DATA, 'test.csv')\n",
    "client_data_path = os.path.join(DATA, 'cliente_tabla.csv')\n",
    "product_data_path = os.path.join(DATA, 'producto_tabla.csv')\n",
    "town_state_path = os.path.join(DATA, 'town_state.csv')\n",
    "\n",
    "print('Loading data..')\n",
    "df_train = load_data(path=DATA, file_name='train.csv', nrows=10**7)\n",
    "df_client = load_data(path=DATA, file_name='cliente_tabla.csv')\n",
    "df_prod = load_data(path=DATA, file_name='producto_tabla.csv')\n",
    "df_town = load_data(path=DATA, file_name='town_state.csv')\n",
    "df_test = load_data(path=DATA, file_name='test.csv')\n",
    "df_test = df_test.drop(['id'], axis=1)\n",
    "\n",
    "target = 'Demanda_uni_equil'\n",
    "indep_vars = list(df_test.columns.values)\n",
    "print(indep_vars)\n",
    "y = df_train['Demanda_uni_equil']\n",
    "X = df_train[indep_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gbm(X_train,  y_train, grid_search=False, verbose=True,\n",
    "              min_samples_split=None, min_samples_leaf=50, max_depth=8,\n",
    "              max_features='sqrt', sub_sample=0.8, n_estimators=100,\n",
    "              learning_rate=0.1, random_state=10, param_grid=None,\n",
    "              ):\n",
    "    \"\"\"\n",
    "\n",
    "    :param X_train:\n",
    "    :param indep_vars:\n",
    "    :param dep_var:\n",
    "    :param verbose:\n",
    "    :param min_samples_leaf: (int) prevent overfitting, intuition based value..\n",
    "    :param max_depth: (int) 8 # 5 -8, based on number of features and dataset size\n",
    "    :param max_features: (str) 'sqrt' # general rule of thumb: sqrt(n_samples)\n",
    "    :param sub_sample: (float) fraction of observations to be selected for each tree (0.8 commonly used value)\n",
    "    :param n_estimators: (int) number of sequential trees to be modeled\n",
    "    :param learning_rate: (float)\n",
    "    :param random_state: (int)\n",
    "    :param param_grid: (dict)\n",
    "    :return:\n",
    "            (model)\n",
    "    \"\"\"\n",
    "    n_samples = X_train.shape[0]\n",
    "    if not min_samples_split:\n",
    "        min_samples_split = n_samples * .01 # prevent overfitting, general rule of thumb: 0.5 - 1%\n",
    "\n",
    "    gbm = GradientBoostingClassifier(\n",
    "        min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n",
    "        max_depth=max_depth, max_features=max_features, learning_rate=learning_rate,\n",
    "        n_estimators=n_estimators, subsample=sub_sample, random_state=random_state)\n",
    "    model = gbm\n",
    "    if grid_search:\n",
    "        if not param_grid:\n",
    "            param_grid = {'n_estimators':range(20,81,10)}\n",
    "        model = GridSearchCV(\n",
    "            estimator=gbm,\n",
    "            param_grid=param_grid,\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=4,\n",
    "            iid=False,\n",
    "            cv=5)\n",
    "    model.fit(X_train, y_train)\n",
    "    if grid_search and verbose:\n",
    "        print('Model Grid scores: {0}, best params: {1}, best score: {2}'\n",
    "              .format(model.grid_scores_, model.best_params_, model.best_score_))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbm_predict(X_train, y_train, X_test, y_test, indep_vars, grid_search=True,\n",
    "                cv=True, verbose=True, cv_folds=5, scoring='roc_auc'):\n",
    "\n",
    "    # Train\n",
    "    model = train_gbm(X_train,  y_train, grid_search=grid_search, verbose=verbose)\n",
    "    # predict on train\n",
    "    train_pred = model.predict(X_test)\n",
    "    # probability\n",
    "    train_pred_prob = model.predict_prob(X_test)[:, 1]\n",
    "\n",
    "    # Cross-validation\n",
    "    if cv:\n",
    "        cv_score = cross_validation.cross_val_score(model, X_test,\n",
    "                                                    y_test, cv=cv_folds,\n",
    "                                                    scoring=scoring)\n",
    "    if verbose:\n",
    "        print(\"\\nModel Report\")\n",
    "        print(\"Accuracy : %.4g\" % metrics.accuracy_score(X_test.values, train_pred))\n",
    "        print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(X_test, train_pred_prob))\n",
    "        if cv:\n",
    "            print(\"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\"\n",
    "                  % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n",
    "        # feature relevance\n",
    "        predictive_relavance = pd.Series(model.feature_importances_, indep_vars).sort_values(ascending=False)\n",
    "        predictive_relavance.plot(kind='bar', title='Feature relevance')\n",
    "        plt.ylabel('Feature relevance score')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the big guns - enter spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize a configuration object ..\n",
    "MY_NAME='diogo' # change this\n",
    "conf = SparkConf().setMaster(\"yarn-client\").setAppName(\"bimbo - {}\".format(MY_NAME)) \\\n",
    ".set(\"spark.driver.memory\", \"2g\").set(\"spark.executor.memory\", \"2g\") \\\n",
    ".set(\"spark.executor.instances\", \"2\").set(\"spark.dynamicAllocation.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .. and pass it to a new SparkContext \n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "#initialize hiveContext\n",
    "#sqlContext = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train = 'mnt/tests/train.csv'\n",
    "s3_test = 'mnt/tests/test.csv'\n",
    "s3_client = 'mnt/tests/cliente_tabla.csv'\n",
    "s3_prod = 'mnt/tests/producto_tabla.csv'\n",
    "s3_town = 'mnt/tests/town_state.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading datasets from hdfs')\n",
    "print('Loading train set...')\n",
    "df_train = sqlContext.read \\\n",
    "    .format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(s3_train)\n",
    "#df_train.select(\"*\").write.save(\"{}.parquet\".format(\"train_bimbo\"), format=\"parquet\")\n",
    "print('finished loading')\n",
    "df_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading test set...')\n",
    "df_test = sqlContext.read \\\n",
    "    .format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(s3_test)\n",
    "#df_test.select(\"*\").write.save(\"{}.parquet\".format(\"test_bimbo\"), format=\"parquet\")\n",
    "print('finished loading')\n",
    "df_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading client set...')\n",
    "df_client = sqlContext.read \\\n",
    "    .format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(s3_client)\n",
    "#df_client.select(\"*\").write.save(\"{}.parquet\".format(\"client_bimbo\"), format=\"parquet\")\n",
    "print('finished loading')\n",
    "df_client.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prod = sqlContext.read \\\n",
    "    .format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(s3_prod)   \n",
    "#df_prod.select(\"*\").write.save(\"{}.parquet\".format(\"prod_bimbo\"), format=\"parquet\")\n",
    "print('finished loading')\n",
    "df_prod.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_town = sqlContext.read \\\n",
    "    .format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(s3_town)\n",
    "#df_town.select(\"*\").write.save(\"{}.parquet\".format(\"town_bimbo\"), format=\"parquet\")\n",
    "print('finished loading')\n",
    "df_town.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Showing head')\n",
    "df_train.printSchema()\n",
    "df_train.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading xgboost package..\n"
     ]
    }
   ],
   "source": [
    "print('Loading xgboost package..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you setup your env correctly: https://github.com/dmlc/xgboost/blob/master/doc/build.md#python-package-installation\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = os.path.curdir\n",
    "print('Current directory is: {0}'.format(curr_dir))\n",
    "data_path = os.path.join(curr_dir, 'data')\n",
    "train_path = os.path.join(data_path, 'train.csv')\n",
    "test_path = os.path.join(data_path, 'test.csv')\n",
    "client_data_path = os.path.join(data_path, 'cliente_tabla.csv')\n",
    "product_data_path = os.path.join(data_path, 'producto_tabla.csv')\n",
    "town_state_path = os.path.join(data_path, 'town_state.csv')\n",
    "print('Reading data from {0}...'.format(curr_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Reading data from {0}...'.format(curr_dir))\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "df_client = pd.read_csv(client_data_path)\n",
    "df_prod = pd.read_csv(product_data_path)\n",
    "print('Finished loading data.')\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}